{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hgtk\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import keras\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "def int_sentence(left_texts, right_texts, vocab_index):\n",
    "    left_int = []\n",
    "    right_int = []\n",
    "\n",
    "    for i in range(len(left_texts)):\n",
    "        left_etc = []\n",
    "        right_etc = []\n",
    "        for j in range(len(left_texts[i])):\n",
    "            left_etc.append(vocab_index[left_texts[i][j]])\n",
    "        for j in range(len(right_texts[i])):\n",
    "            right_etc.append(vocab_index[right_texts[i][j]])\n",
    "        left_int.append(left_etc)\n",
    "        right_int.append(right_etc)\n",
    "        \n",
    "    return left_int, right_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = open('filename1', 'r', encoding='utf-8-sig')\n",
    "train = open('filename2', 'r', encoding='utf-8-sig')\n",
    "cv = open('filename3', 'r', encoding='utf-8-sig')\n",
    "test = open('filename4', 'r', encoding='utf-8-sig')\n",
    "\n",
    "all_sentence = []\n",
    "\n",
    "for line in All:\n",
    "    line = line.split('\\t')\n",
    "    left = hgtk.text.decompose(line[0])\n",
    "    right = hgtk.text.decompose(line[1])\n",
    "    \n",
    "    etc = []\n",
    "    for i in left:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    all_sentence.append(etc)\n",
    "    \n",
    "    etc = []\n",
    "    for i in right:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    all_sentence.append(etc)\n",
    "    \n",
    "\n",
    "max_len = max([len(i) for i in all_sentence])\n",
    "\n",
    "vocab = set()\n",
    "for line in all_sentence:\n",
    "    for word in line:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)+1\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "vocab_index = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_index[vocab[i]] = len(vocab_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_sen = []\n",
    "train_right_sen = []\n",
    "train_label = []\n",
    "\n",
    "for line in train:\n",
    "    line = line.split('\\t')\n",
    "    left = hgtk.text.decompose(line[0])\n",
    "    right = hgtk.text.decompose(line[1])\n",
    "    \n",
    "    etc = []\n",
    "    for i in left:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    train_left_sen.append(etc)\n",
    "    \n",
    "    etc = []\n",
    "    for i in right:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    train_right_sen.append(etc)\n",
    "    train_label.append(line[2].strip())\n",
    "\n",
    "cv_left_sen = []\n",
    "cv_right_sen = []\n",
    "cv_label = []\n",
    "\n",
    "for line in cv:\n",
    "    line = line.split('\\t')\n",
    "    left = hgtk.text.decompose(line[0])\n",
    "    right = hgtk.text.decompose(line[1])\n",
    "    \n",
    "    etc = []\n",
    "    for i in left:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    cv_left_sen.append(etc)\n",
    "    \n",
    "    etc = []\n",
    "    for i in right:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    cv_right_sen.append(etc)\n",
    "    cv_label.append(line[2].strip())\n",
    "\n",
    "test_left_sen = []\n",
    "test_right_sen = []\n",
    "test_label = []\n",
    "\n",
    "for line in test:\n",
    "    line = line.split('\\t')\n",
    "    left = hgtk.text.decompose(line[0])\n",
    "    right = hgtk.text.decompose(line[1])\n",
    "    \n",
    "    etc = []\n",
    "    for i in left:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    test_left_sen.append(etc)\n",
    "    \n",
    "    etc = []\n",
    "    for i in right:\n",
    "        if i != 'ᴥ':\n",
    "            etc.append(i + ' ')\n",
    "    test_right_sen.append(etc)\n",
    "    test_label.append(line[2].strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_left_int, train_right_int = int_sentence(train_left_sen, train_right_sen, vocab_index)\n",
    "cv_left_int, cv_right_int = int_sentence(cv_left_sen, cv_right_sen, vocab_index)\n",
    "test_left_int, test_right_int = int_sentence(test_left_sen, test_right_sen, vocab_index)\n",
    "\n",
    "train_left = pad_sequences(train_left_int, padding='post', maxlen=max_len)\n",
    "train_right = pad_sequences(train_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "cv_left = pad_sequences(cv_left_int, padding='post', maxlen=max_len)\n",
    "cv_right = pad_sequences(cv_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "test_left = pad_sequences(test_left_int, padding='post', maxlen=max_len)\n",
    "test_right = pad_sequences(test_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "train_label = keras.utils.to_categorical(train_label)\n",
    "cv_label = keras.utils.to_categorical(cv_label)\n",
    "test_label = keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 304)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 304)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 304, 300)     41100       input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 304, 256)     230656      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 304, 256)     307456      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 304, 256)     384256      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 304, 100)     122800      conv1d_1[0][0]                   \n",
      "                                                                 conv1d_1[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 304, 100)     122800      conv1d_2[0][0]                   \n",
      "                                                                 conv1d_2[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 304, 100)     122800      conv1d_3[0][0]                   \n",
      "                                                                 conv1d_3[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_1 (SeqSelfAt (None, 304, 100)     6465        bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_3 (SeqSelfAt (None, 304, 100)     6465        bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_5 (SeqSelfAt (None, 304, 100)     6465        bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_2 (SeqSelfAt (None, 304, 100)     6465        bidirectional_1[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_4 (SeqSelfAt (None, 304, 100)     6465        bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "seq_self_attention_6 (SeqSelfAt (None, 304, 100)     6465        bidirectional_3[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 30400)        0           seq_self_attention_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 30400)        0           seq_self_attention_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 30400)        0           seq_self_attention_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 30400)        0           seq_self_attention_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 30400)        0           seq_self_attention_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 30400)        0           seq_self_attention_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 182400)       0           flatten_1[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_5[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 2)            364802      concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,735,460\n",
      "Trainable params: 1,735,460\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_input = layers.Input(shape=(max_len,))\n",
    "right_input = layers.Input(shape=(max_len,))\n",
    "\n",
    "embedded_layer = layers.Embedding(vocab_size, 300)\n",
    "\n",
    "left_emd = embedded_layer(left_input)\n",
    "right_emd = embedded_layer(right_input)\n",
    "\n",
    "cnn3 = layers.Conv1D(filters=256, kernel_size=3, padding='same', activation='relu')\n",
    "cnn4 = layers.Conv1D(filters=256, kernel_size=4, padding='same', activation='relu')\n",
    "cnn5 = layers.Conv1D(filters=256, kernel_size=5, padding='same', activation='relu')\n",
    "\n",
    "left_cnn3 = cnn3(left_emd)\n",
    "right_cnn3 = cnn3(right_emd)\n",
    "\n",
    "left_cnn4 = cnn4(left_emd)\n",
    "right_cnn4 = cnn4(right_emd)\n",
    "\n",
    "left_cnn5 = cnn5(left_emd)\n",
    "right_cnn5 = cnn5(right_emd)\n",
    "\n",
    "lstm3 = layers.Bidirectional(layers.LSTM(50, return_sequences=True))\n",
    "lstm4 = layers.Bidirectional(layers.LSTM(50, return_sequences=True))\n",
    "lstm5 = layers.Bidirectional(layers.LSTM(50, return_sequences=True))\n",
    "\n",
    "left_lstm3 = lstm3(left_cnn3)\n",
    "right_lstm3 = lstm3(right_cnn3)\n",
    "\n",
    "left_lstm4 = lstm4(left_cnn4)\n",
    "right_lstm4 = lstm4(right_cnn4)\n",
    "\n",
    "left_lstm5 = lstm5(left_cnn5)\n",
    "right_lstm5 = lstm5(right_cnn5)\n",
    "\n",
    "left_attention3 = SeqSelfAttention(attention_activation='tanh')(left_lstm3)\n",
    "right_attention3 = SeqSelfAttention(attention_activation='tanh')(right_lstm3)\n",
    "\n",
    "left_attention4 = SeqSelfAttention(attention_activation='tanh')(left_lstm4)\n",
    "right_attention4 = SeqSelfAttention(attention_activation='tanh')(right_lstm4)\n",
    "\n",
    "left_attention5 = SeqSelfAttention(attention_activation='tanh')(left_lstm5)\n",
    "right_attention5 = SeqSelfAttention(attention_activation='tanh')(right_lstm5)\n",
    "\n",
    "left_f3 = layers.Flatten()(left_attention3)\n",
    "right_f3 = layers.Flatten()(right_attention3)\n",
    "\n",
    "left_f4 = layers.Flatten()(left_attention4)\n",
    "right_f4 = layers.Flatten()(right_attention4)\n",
    "\n",
    "left_f5 = layers.Flatten()(left_attention5)\n",
    "right_f5 = layers.Flatten()(right_attention5)\n",
    "\n",
    "concat_layer = layers.Concatenate(axis=1)([left_f3, left_f4, left_f5, right_f3, right_f4, right_f5])\n",
    "\n",
    "outputs = layers.Dense(2, activation='softmax')(concat_layer)\n",
    "\n",
    "model = Model(inputs=[left_input, right_input], outputs=[outputs])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8800 samples, validate on 1100 samples\n",
      "Epoch 1/100\n",
      "8800/8800 [==============================] - 712s 81ms/step - loss: 0.6981 - acc: 0.5795 - val_loss: 0.6379 - val_acc: 0.5973\n",
      "Epoch 2/100\n",
      "8800/8800 [==============================] - 682s 77ms/step - loss: 0.6058 - acc: 0.6636 - val_loss: 0.6109 - val_acc: 0.7091\n",
      "Epoch 3/100\n",
      "8800/8800 [==============================] - 683s 78ms/step - loss: 0.5739 - acc: 0.6934 - val_loss: 0.5829 - val_acc: 0.6964\n",
      "Epoch 4/100\n",
      "8800/8800 [==============================] - 685s 78ms/step - loss: 0.5169 - acc: 0.7385 - val_loss: 0.5126 - val_acc: 0.7427\n",
      "Epoch 5/100\n",
      "8800/8800 [==============================] - 682s 78ms/step - loss: 0.4798 - acc: 0.7730 - val_loss: 0.5012 - val_acc: 0.7609\n",
      "Epoch 6/100\n",
      "8800/8800 [==============================] - 687s 78ms/step - loss: 0.4292 - acc: 0.8043 - val_loss: 0.4870 - val_acc: 0.7836\n",
      "Epoch 7/100\n",
      "8800/8800 [==============================] - 692s 79ms/step - loss: 0.3675 - acc: 0.8376 - val_loss: 0.4545 - val_acc: 0.7909\n",
      "Epoch 8/100\n",
      "8800/8800 [==============================] - 684s 78ms/step - loss: 0.3180 - acc: 0.8650 - val_loss: 0.6274 - val_acc: 0.7218\n",
      "Epoch 9/100\n",
      "8800/8800 [==============================] - 689s 78ms/step - loss: 0.2736 - acc: 0.8845 - val_loss: 0.5155 - val_acc: 0.7982\n",
      "Epoch 10/100\n",
      "8800/8800 [==============================] - 686s 78ms/step - loss: 0.2276 - acc: 0.9076 - val_loss: 0.5555 - val_acc: 0.8109\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00010: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f65ab649048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.utils.multi_gpu_model(model, gpus=3)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit([train_left, train_right], [train_label], batch_size=64, epochs=100, validation_data=([cv_left, cv_right], [cv_label]), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100/1100 [==============================] - 122s 111ms/step\n",
      "Accuracy: 0.8109090911258351\n",
      "Loss: 0.4441912594166669\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate([test_left, test_right], [test_label])\n",
    "\n",
    "print('Accuracy: '+str(evaluation[1]))\n",
    "print('Loss: '+str(evaluation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
